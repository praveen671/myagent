agent.py

from typing import TypedDict
from langgraph.graph import StateGraph
from openai import AzureOpenAI
from config import AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_DEPLOYMENT
from logging_utils import log_event, log_exception, get_correlation_id
from typing import List

# Define state schema â€” match the keys you use in ai_agent_node
class AgentState(TypedDict):
    input: str
    output: str
    followups: List[str]




# Initialize LangGraph with schema
graph = StateGraph(AgentState)

# Azure OpenAI client
client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version="2024-05-01-preview"
)


def ai_agent_node(state: AgentState):
    correlation_id = get_correlation_id()
    log_event("AgentRequest", {"correlation_id": correlation_id, "input": state["input"]})
    try:
        # Main answer
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[{"role": "user", "content": state["input"]}]
        )
        output = response.choices[0].message.content

        # Generate follow-up questions
        followup_prompt = f"Based on this answer: '{output}', suggest 3 relevant follow-up questions."
        followup_response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "you person who to response after reviewing,you thinking of  relevant follow-up questions."},
                {"role": "user", "content": followup_prompt}
            ],
            temperature=0.7
        )
        followups_text = followup_response.choices[0].message.content
        followups = [q.strip("- ").strip() for q in followups_text.split("\n") if q.strip()]

        log_event("AgentResponse", {"correlation_id": correlation_id, "output": output, "followups": followups})
        return {"input": state["input"], "output": output, "followups": followups}

    except Exception as e:
        log_exception(e)

    
graph.add_node("ai_agent", ai_agent_node)
graph.set_entry_point("ai_agent")  # Define starting node
compiled_graph = graph.compile()   # Compile the workflow


api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from agent import compiled_graph
from logging_utils import log_event, log_exception, get_correlation_id
from typing import List

app = FastAPI()

class AgentRequest(BaseModel):
    input: str

class AgentResponse(BaseModel):
    status: str
    answer: str
    followups: List[str]
    



@app.post("/agent")
async def run_agent(request: AgentRequest):
    correlation_id = get_correlation_id()
    log_event("APIRequest", {"correlation_id": correlation_id, "input": request.input})
    try:
        # Invoke the compiled LangGraph workflow
        result = compiled_graph.invoke({"input": request.input})
        output = result.get("output", "No output")
        followups = result.get("followups", "No followups")

        log_event("APIResponse", {"correlation_id": correlation_id, "output": output})
        return AgentResponse(status="success", answer=output, followups=followups)
    except Exception as e:
        log_exception(e)
        raise HTTPException(status_code=500, answer=e)

@app.get("/")
def root():
    return {"message": "API is running"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



config.py

import os

# Configuration for Azure OpenAI
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-05-01-preview")

# Application Insights / logging
APPINSIGHTS_CONNECTION_STRING = os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING")
APPINSIGHTS_INSTRUMENTATIONKEY = os.getenv("APPINSIGHTS_INSTRUMENTATIONKEY")

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")
APP_INSIGHTS_CONN_STRING = os.getenv("APP_INSIGHTS_CONN_STRING")
AGENT_API_URL = os.getenv("AGENT_API_URL", "http://localhost:8000/agent")

logging_utils.py

import logging
import contextvars
from typing import Any, Dict, Optional

try:
    from applicationinsights import TelemetryClient
except Exception:
    TelemetryClient = None

from config import APPINSIGHTS_CONNECTION_STRING, APPINSIGHTS_INSTRUMENTATIONKEY

_correlation_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("correlation_id", default=None)

logger = logging.getLogger("sample_agent")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(ch)


class _Telemetry:
    def __init__(self):
        self._client = None
        key = APPINSIGHTS_CONNECTION_STRING or APPINSIGHTS_INSTRUMENTATIONKEY
        if TelemetryClient and key:
            try:
                self._client = TelemetryClient(key)
            except Exception:
                self._client = None

    def track_event(self, name: str, properties: Optional[Dict[str, Any]] = None):
        if self._client:
            self._client.track_event(name, properties or {})
            self._client.flush()

    def track_exception(self, exc: Exception, properties: Optional[Dict[str, Any]] = None):
        if self._client:
            try:
                self._client.track_exception(exception=exc, properties=properties or {})
                self._client.flush()
            except Exception:
                pass


_telemetry = _Telemetry()


def set_correlation_id(cid: str):
    _correlation_id.set(cid)


def get_correlation_id() -> Optional[str]:
    return _correlation_id.get()


def log_event(name: str, properties: Optional[Dict[str, Any]] = None):
    cid = get_correlation_id()
    props = dict(properties or {})
    if cid:
        props.setdefault("correlation_id", cid)
    logger.info("%s %s", name, props)
    try:
        _telemetry.track_event(name, props)
    except Exception:
        pass


def log_exception(exc: Exception, properties: Optional[Dict[str, Any]] = None):
    cid = get_correlation_id()
    props = dict(properties or {})
    if cid:
        props.setdefault("correlation_id", cid)
    logger.exception("Exception: %s - %s", exc, props)
    try:
        _telemetry.track_exception(exc, props)
    except Exception:
        pass
from applicationinsights import TelemetryClient
import uuid
import os


instrumentation_key = os.getenv("APP_INSIGHTS_CONN_STRING")  # This should be just the key
tc = TelemetryClient(instrumentation_key)

def log_event(name, properties=None):
    tc.track_event(name, properties)
    tc.flush()

def log_exception(exception):
    tc.track_exception(exception)
    tc.flush()

def get_correlation_id():
    return str(uuid.uuid4())



"""
Simple LangChain Agent Example - All in One File
This demonstrates a basic agent with tools, following LangChain v1.x best practices.
"""
from __future__ import annotations

import json
from typing import Any

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_openai import AzureChatOpenAI
import os
from dotenv import load_dotenv

# ==============================================================================
# Section 1: Configuration
# ==============================================================================

# Load environment variables from .env file
load_dotenv()

AZURE_CONFIG = {
    "azure_endpoint": os.getenv("AZURE_OPENAI_ENDPOINT"),
    "api_key": os.getenv("AZURE_OPENAI_API_KEY"),
    "api_version": os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
    "azure_deployment": os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    "model": os.getenv("AZURE_OPENAI_MODEL", "gpt-4o-mini"),
    "temperature": float(os.getenv("AZURE_OPENAI_TEMPERATURE", "0.1")),
}


# ==============================================================================
# Section 2: Tool Definitions
# ==============================================================================

@tool
def calculate_sum(numbers: list[float]) -> float:
    """Calculate the sum of a list of numbers.
    
    Args:
        numbers: A list of numbers to sum together.
        
    Returns:
        The sum of all numbers in the list.
    """
    print("method invocked" )
    return sum(numbers)


@tool
def get_word_count(text: str) -> int:
    """Count the number of words in a text string.
    
    Args:
        text: The text to count words in.
        
    Returns:
        The number of words in the text.
    """
    return len(text.split())


@tool
def reverse_text(text: str) -> str:
    """Reverse the characters in a text string.
    
    Args:
        text: The text to reverse.
        
    Returns:
        The reversed text.
    """
    return text[::-1]


@tool
def create_json_object(key: str, value: Any) -> str:
    """Create a JSON object with a single key-value pair.
    
    Args:
        key: The key name for the JSON object.
        value: The value to store (can be string, number, list, or dict).
        
    Returns:
        A JSON string representation of the object.
    """
    return json.dumps({key: value}, indent=2)


# ==============================================================================
# Section 3: Agent Class
# ==============================================================================

class SimpleAgent:
    """A simple LangChain agent with tool calling capabilities."""
    
    def __init__(self, azure_config: dict[str, Any]) -> None:
        """Initialize the agent with Azure OpenAI configuration.
        
        Args:
            azure_config: Dictionary containing Azure OpenAI configuration.
        """
        self.llm = AzureChatOpenAI(**azure_config)
        
        # Register all tools
        self.tools = [
            calculate_sum,
            get_word_count,
            reverse_text,
            create_json_object,
        ]
        
        # Create tool map for quick lookup
        self.tool_map = {tool.name: tool for tool in self.tools}
        
        # Bind tools to the LLM
        self.tool_llm = self.llm.bind_tools(self.tools)
        
        # System prompt
        self.system_prompt = (
            "You are a helpful AI assistant with access to several tools. "
            "Use the available tools to help answer user questions accurately. "
            "Always use tools when they are relevant to the user's request."
        )
        
        # Conversation history
        self.history: list[HumanMessage | AIMessage] = []
    
    def _execute_tool(self, tool_name: str, args: dict[str, Any]) -> str:
        """Execute a tool by name with the provided arguments.
        
        Args:
            tool_name: Name of the tool to execute.
            args: Dictionary of arguments to pass to the tool.
            
        Returns:
            The tool's output as a string.
        """
        tool = self.tool_map.get(tool_name)
        if not tool:
            return f"Error: Tool '{tool_name}' not found."
        
        try:
            result = tool.invoke(args)
            return str(result)
        except Exception as e:
            return f"Error executing tool '{tool_name}': {str(e)}"
    
    def chat(self, user_message: str) -> str:
        """Process a user message and return the agent's response.
        
        Args:
            user_message: The user's input message.
            
        Returns:
            The agent's response after potentially using tools.
        """
        # Build conversation
        conversation: list[SystemMessage | HumanMessage | AIMessage | ToolMessage] = [
            SystemMessage(content=self.system_prompt)
        ]
        conversation.extend(self.history)
        conversation.append(HumanMessage(content=user_message))
        
        # Get initial response from LLM
        ai_message = self.tool_llm.invoke(conversation)
        print(f"ğŸ¤– LLM wants to use tools: {bool(getattr(ai_message, 'tool_calls', None))}")
        
        # Tool calling loop
        while getattr(ai_message, "tool_calls", None):
            print(f"ğŸ”§ Executing {len(ai_message.tool_calls)} tool call(s)...")
            
            tool_messages: list[ToolMessage] = []
            for call in ai_message.tool_calls:
                tool_name = call.get("name")
                tool_args = call.get("args", {})
                
                print(f"   â†’ Tool: {tool_name}, Args: {tool_args}")
                
                # Execute the tool
                observation = self._execute_tool(tool_name, tool_args)
                print(f"   âœ“ Result: {observation[:100]}...")
                
                # Create tool message
                tool_messages.append(
                    ToolMessage(content=observation, tool_call_id=call["id"])
                )
            
            # Add messages to conversation
            conversation.append(ai_message)
            conversation.extend(tool_messages)
            
            # Get next response from LLM
            ai_message = self.tool_llm.invoke(conversation)
        
        # Save to history
        self.history.append(HumanMessage(content=user_message))
        self.history.append(ai_message)
        
        # Return final response
        return ai_message.content if isinstance(ai_message.content, str) else str(ai_message.content)
    
    def clear_history(self) -> None:
        """Clear the conversation history."""
        self.history.clear()
        print("âœ“ Conversation history cleared.")


# ==============================================================================
# Section 4: Example Usage
# ==============================================================================

def main() -> None:
    """Main function demonstrating the agent's capabilities."""
    
    print("=" * 70)
    print("Simple LangChain Agent Demo")
    print("=" * 70)
    
    # Create agent
    agent = SimpleAgent(AZURE_CONFIG)
    
    # Example 1: Math calculation
    print("\nğŸ“ Example 1: Math Calculation")
    print("-" * 70)
    response = agent.chat("What is the sum of 10, 25, 33, and 47?")
    print(f"Agent: {response}\n")
    
    # Example 2: Text processing
    print("\nğŸ“ Example 2: Text Processing")
    print("-" * 70)
    response = agent.chat("How many words are in the sentence: 'LangChain makes building agents easy'?")
    print(f"Agent: {response}\n")
    
    # Example 3: String manipulation
    print("\nğŸ“ Example 3: String Manipulation")
    print("-" * 70)
    response = agent.chat("Reverse the text 'Hello World'")
    print(f"Agent: {response}\n")
    
    # Example 4: JSON creation
    print("\nğŸ“ Example 4: JSON Creation")
    print("-" * 70)
    response = agent.chat("Create a JSON object with key 'status' and value 'success'")
    print(f"Agent: {response}\n")
    
    # Example 5: Multiple tool usage
    print("\nğŸ“ Example 5: Multiple Tools")
    print("-" * 70)
    response = agent.chat(
        "Calculate the sum of 5, 10, 15 and then tell me how many words are in 'AI agents are powerful tools'"
    )
    print(f"Agent: {response}\n")
    
    # Clear history
    agent.clear_history()
    
    print("=" * 70)
    print("Demo completed!")
    print("=" * 70)


if __name__ == "__main__":
    main()





